{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DvRcQIxISJ30"
   },
   "source": [
    "# **Homework 9: Variational Autoencoders**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tJQ_deIUSca8"
   },
   "source": [
    "## **About**\n",
    "\n",
    "### **Due**\n",
    "\n",
    "Monday 4/22/19, 11:59 PM CST\n",
    "\n",
    "### **Goal**\n",
    "\n",
    "This homework focuses on creating variational autoencoders applied to the MNIST dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "d_TOu0F9Sze-"
   },
   "source": [
    "## Dev Environment\n",
    "### Working on Google Colab\n",
    "You may choose to work locally or on Google Colaboratory. You have access to free compute through this service. \n",
    "1. Visit https://colab.research.google.com/drive \n",
    "2. Navigate to the **`Upload`** tab, and upload your `HW10.ipynb`\n",
    "3. Now on the top right corner, under the `Comment` and `Share` options, you should see a `Connect` option. Once you are connected, you will have access to a VM with 12GB RAM, 50 GB disk space and a single GPU. The dropdown menu will allow you to connect to a local runtime as well.\n",
    "\n",
    "**Notes:** \n",
    "* **If you do not have a working setup for Python 3, this is your best bet. It will also save you from heavy installations like `tensorflow` if you don't want to deal with those.**\n",
    "* ***There is a downside*. You can only use this instance for a single 12-hour stretch, after which your data will be deleted, and you would have redownload all your datasets, any libraries not already on the VM, and regenerate your logs**.\n",
    "\n",
    "\n",
    "### Installing PyTorch and Dependencies\n",
    "\n",
    "The instructions for installing and setting up PyTorch can be found at https://pytorch.org/get-started/locally/. Make sure you follow the instructions for your machine. For any of the remaining libraries used in this assignment:\n",
    "* We have provided a `hw8_requirements.txt` file on the homework web page. \n",
    "* Download this file, and in the same directory you can run `pip3 install -r hw8_requirements.txt`\n",
    "\n",
    "Check that PyTorch installed correctly by running the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YB1d0Rm6SWiB"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.6629, 0.1561, 0.8727],\n",
       "        [0.3793, 0.6569, 0.2538],\n",
       "        [0.9752, 0.2465, 0.4404],\n",
       "        [0.2720, 0.3417, 0.3627],\n",
       "        [0.5743, 0.8157, 0.1257]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "torch.rand(5, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4dLgW4qRTEqj"
   },
   "source": [
    "The output should look something like\n",
    "\n",
    "```python\n",
    "tensor([[0.3380, 0.3845, 0.3217],\n",
    "        [0.8337, 0.9050, 0.2650],\n",
    "        [0.2979, 0.7141, 0.9069],\n",
    "        [0.1449, 0.1132, 0.1375],\n",
    "        [0.4675, 0.3947, 0.1426]])\n",
    "```\n",
    "\n",
    "### Let's get started with the assignment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ysPO_A4iTJvm"
   },
   "source": [
    "## **Instructions**\n",
    "\n",
    "### **Part 1 - Datasets and Dataloaders**\n",
    "\n",
    "This part of the assignment is similar to HW 8. \n",
    "\n",
    "**Create a directory named hw9_data with the following command.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VFBEkk5aT0Xy"
   },
   "outputs": [],
   "source": [
    "!mkdir hw9_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3UK59qgDT7pU"
   },
   "source": [
    "\n",
    "**Now use `torch.datasets.MNIST` to load the Train and Test data into `hw9_data`.** \n",
    "* ** Use the directory you created above as the `root` directory for your datasets**\n",
    "* ** Populate the `transformations` variable with any transformations you would like to perform on your data.** (Hint: You will need to do at least one)\n",
    "* **Pass your `transformations` variable to `torch.datasets.MNIST`. This allows you to perform arbitrary transformations to your data at loading time.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "acNAlKImT7Ta"
   },
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms\n",
    "CUDA = True\n",
    "## YOUR CODE HERE ##\n",
    "transformations = transforms.Compose([transforms.ToTensor(),transforms.Normalize((0.1307,), (0.3081,))])\n",
    "mnist_train = datasets.MNIST(root='./hw9_data', train=True, download=True, transform=transformations) \n",
    "mnist_test = datasets.MNIST(root='./hw9_data', train=False, download=True, transform=transformations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jH35P-FcULyr"
   },
   "source": [
    "**Any file in our dataset will now be read at runtime, and the specified transformations we need on it will be applied when we need it.**. \n",
    "\n",
    "We could iterate through these directly using a loop, but this is not idiomatic. PyTorch provides us with this abstraction in the form of `DataLoaders`. The module of interest is `torch.utils.data.DataLoader`. \n",
    "\n",
    "`DataLoader` allows us to do lots of useful things\n",
    "* Group our data into batches\n",
    "* Shuffle our data\n",
    "* Load the data in parallel using `multiprocessing` workers\n",
    "\n",
    "**Use `DataLoader` to create a loader for the training set and one for the testing set**\n",
    "* **Use a `batch_size` of 32 to start, you may change it if you wish.**\n",
    "* **Set the `shuffle` parameter to `True`.** \n",
    "\n",
    "**Check that the data was loaded successfully before proceeding to the next sections. **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "q3DO5eriUhZF"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "## YOUR CODE HERE ##\n",
    "train_loader = DataLoader(mnist_train, batch_size=32, shuffle=True, num_workers=4)\n",
    "test_loader = DataLoader(mnist_test, batch_size=32, shuffle=True, num_workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EccGF9s3UgOm"
   },
   "source": [
    "## **Part 2 - Encoder and Decoders (0 points)**\n",
    "\n",
    "In this section we will be creating the encoder and decoder for our variational autoencoder (VAE). \n",
    "\n",
    "You can take a look at the following to understand how VAE's work. \n",
    "\n",
    "*   https://towardsdatascience.com/intuitively-understanding-variational-autoencoders-1bfe67eb5daf\n",
    "*  http://kvfrans.com/variational-autoencoders-explained/\n",
    "*  https://jmetzen.github.io/2015-11-27/vae.html\n",
    "\n",
    "VAEs work around a latent space who's dimension can be chosen by us. We will leave this as a parameter for the Encoder and Decoder classes that you will have to populate. \n",
    "\n",
    "Feel free to use any network architecture that you wish. Try simpler network structures like a few linear layers before trying anything more complicated. \n",
    "\n",
    "### For the Encoder:\n",
    "\n",
    "*   **Finish the __init__() function.**\n",
    "*  **Finish the forward() function.** \n",
    "*  **Assume that input to forward, x, is of shape (batch_size, 28,28)**\n",
    "*  **forward() should return two tensors of size latent_dim like a standard encoder of a VAE**\n",
    "* **One of the tensors should correspond to the mean of the encoding and the other tensor should correspond to the variance. In practice, it is easier to model the output as the log of the variance (logvar) and we will too**\n",
    "\n",
    "### For the Decoder:\n",
    "\n",
    "*   **Finish the __init__() function.**\n",
    "*  **Finish the forward() function.** \n",
    "*  **Assume that input to forward, x, is of shape (batch_size, latent_dim)**\n",
    "*  **forward() should return a tensor of shape (batch_size, 28,28)**\n",
    "* **Make sure that the output lies in the same range as the input to the encoder (Hint: Sigmoid?)**\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NL4wDY2TYVGP"
   },
   "outputs": [],
   "source": [
    "from torch import nn, optim\n",
    "from torch.autograd import Variable\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VAE, self).__init__()\n",
    "\n",
    "        # ENCODER\n",
    "        # 28 x 28 pixels = 784 input pixels, 400 outputs\n",
    "        self.fc1 = nn.Linear(784, 400)\n",
    "        # rectified linear unit layer from 400 to 400\n",
    "        # max(0, x)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc21 = nn.Linear(400, 20)  # mu layer\n",
    "        self.fc22 = nn.Linear(400, 20)  # logvariance layer\n",
    "        # this last layer bottlenecks through ZDIMS connections\n",
    "\n",
    "        # DECODER\n",
    "        # from bottleneck to hidden 400\n",
    "        self.fc3 = nn.Linear(20, 400)\n",
    "        # from hidden 400 to 784 outputs\n",
    "        self.fc4 = nn.Linear(400, 784)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def encode(self, x: Variable) -> (Variable, Variable):\n",
    "        # h1 is [128, 400]\n",
    "        h1 = self.relu(self.fc1(x))  # type: Variable\n",
    "        return self.fc21(h1), self.fc22(h1)\n",
    "\n",
    "    def reparameterize(self, mu: Variable, logvar: Variable) -> Variable:\n",
    "        if self.training:\n",
    "            std = logvar.mul(0.5).exp_()  # type: Variable\n",
    "            eps = Variable(std.data.new(std.size()).normal_())\n",
    "            return eps.mul(std).add_(mu)\n",
    "        else:\n",
    "            return mu\n",
    "\n",
    "    def decode(self, z: Variable) -> Variable:\n",
    "        h3 = self.relu(self.fc3(z))\n",
    "        return self.sigmoid(self.fc4(h3))\n",
    "\n",
    "    def forward(self, x: Variable) -> (Variable, Variable, Variable):\n",
    "        mu, logvar = self.encode(x.view(-1, 784))\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        return self.decode(z), mu, logvar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "I9Su2FOMZG93"
   },
   "source": [
    "## **Part 3: Training and loss functions** (0 points)\n",
    "\n",
    "Recall that the encoder outputs the mean (mu) and the log of the variance (logvar). This implies that the latent vector of the input image follows a gaussian distribution with mean (mu) and standard deviation (e^[0.5\\*logvar]). To decode this information, the decoder needs a sample from this distribution. \n",
    "\n",
    "**Complete the sample function to generate these samples **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4wUHpN7hd-6h"
   },
   "source": [
    "We also need to create the loss function. Assume that x are your input images and x_hat are your reconstructions of these input images, complete the following loss for a VAE. (Hint: You will need to use mu and logvar as well)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "m5T1tOI0exar"
   },
   "source": [
    "In the following we will instantiate an Encoder and Decoder with a latent dimension of 32.\n",
    "\n",
    "We also define a single optimizer that optimizes the parameters of both the Encoder and the Decoder together. Feel free to use any optimizer of your choice. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RwJkFq5cfn0w"
   },
   "source": [
    "Complete the train function that takes input encoder, decoder, train_loader, optimizer, and number of epochs you wish to train your model for.\n",
    "\n",
    "Training will involve:\n",
    "\n",
    "1.   **One epoch is defined as a full pass of your dataset through your model. We choose the number of epochs we wish to train our model for.**\n",
    "2.   **For each batch, use the encoder to generate the mu and logvar.**\n",
    "3. **Sample a latent vector for each image in the batch and feed this to the decoder to generate the decoded images.**\n",
    "4. **Calculate the loss function for this batch.**\n",
    "5. **Now calculate the gradients for each parameter you are optimizing over. (Hint: Your loss function object can do this for you)**\n",
    "6. **Update your model parameters (Hint: The optimizer comes in here)**\n",
    "7. ** Set the gradients in your model to zero for the next batch.**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.utils.data\n",
    "from torch import nn, optim\n",
    "from torch.autograd import Variable\n",
    "from torch.nn import functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "model = VAE()\n",
    "optimizer = Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "LOG_INTERVAL = 1000\n",
    "SEED = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(recon_x, x, mu, logvar) -> Variable:\n",
    "    # how well do input x and output recon_x agree?\n",
    "    BCE = F.binary_cross_entropy(recon_x, x.view(-1, 784))\n",
    "\n",
    "    # KLD is Kullback–Leibler divergence -- how much does one learned\n",
    "    # distribution deviate from another, in this specific case the\n",
    "    # learned distribution from the unit Gaussian\n",
    "\n",
    "    # see Appendix B from VAE paper:\n",
    "    # Kingma and Welling. Auto-Encoding Variational Bayes. ICLR, 2014\n",
    "    # https://arxiv.org/abs/1312.6114\n",
    "    # - D_{KL} = 0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)\n",
    "    # note the negative D_{KL} in appendix B of the paper\n",
    "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    # Normalise by same number of elements as in reconstruction\n",
    "    KLD /= BATCH_SIZE * 784\n",
    "\n",
    "    # BCE tries to make our reconstruction as accurate as possible\n",
    "    # KLD tries to push the distributions as close as possible to unit Gaussian\n",
    "    return BCE + KLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    # toggle model to train mode\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    # in the case of MNIST, len(train_loader.dataset) is 60000\n",
    "    # each `data` is of BATCH_SIZE samples and has shape [128, 1, 28, 28]\n",
    "    for batch_idx, (data, _) in enumerate(train_loader):\n",
    "        data = Variable(data)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # push whole batch of data through VAE.forward() to get recon_loss\n",
    "        recon_batch, mu, logvar = model(data)\n",
    "        # calculate scalar loss\n",
    "        loss = loss_function(recon_batch, data, mu, logvar)\n",
    "        # calculate the gradient of the loss w.r.t. the graph leaves\n",
    "        # i.e. input variables -- by the power of pytorch!\n",
    "        loss.backward()\n",
    "        train_loss += loss.item()\n",
    "        optimizer.step()\n",
    "        if batch_idx % LOG_INTERVAL == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader),\n",
    "                loss.item() / len(data)))\n",
    "\n",
    "    print('====> Epoch: {} Average loss: {:.4f}'.format(epoch, train_loss / len(train_loader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "F17NVLO8hkP8"
   },
   "outputs": [],
   "source": [
    "def test(epoch):\n",
    "    # toggle model to test / inference mode\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "\n",
    "    # each data is of BATCH_SIZE (default 128) samples\n",
    "    for i, (data, _) in enumerate(test_loader):\n",
    "        # we're only going to infer, so no autograd at all required: volatile=True\n",
    "        data = Variable(data, volatile=True)\n",
    "        recon_batch, mu, logvar = model(data)\n",
    "        test_loss += loss_function(recon_batch, data, mu, logvar).item()\n",
    "        if i == 0:\n",
    "          n = min(data.size(0), 8)\n",
    "          # for the first 128 batch of the epoch, show the first 8 input digits\n",
    "          # with right below them the reconstructed output digits\n",
    "          comparison = torch.cat([data[:n],\n",
    "                                  recon_batch.view(BATCH_SIZE, 1, 28, 28)[:n]])\n",
    "          save_image(comparison.data.cpu(),\n",
    "                     'results/reconstruction_' + str(epoch) + '.png', nrow=n)\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('====> Test set loss: {:.4f}'.format(test_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dLIjuvFkhuxH"
   },
   "source": [
    "Finally call train with the relevant parameters.\n",
    "\n",
    "Note : This function may take a while to complete if you're training for many epochs on a cpu. This is where it comes in handy to be running on Google Colab, or just have a GPU on hand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "puFTbp4Nh2Hb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: -0.605457\n",
      "Train Epoch: 1 [320/60000 (1%)]\tLoss: -0.595299\n",
      "Train Epoch: 1 [640/60000 (1%)]\tLoss: -0.619292\n",
      "Train Epoch: 1 [960/60000 (2%)]\tLoss: -0.622687\n",
      "Train Epoch: 1 [1280/60000 (2%)]\tLoss: -0.644805\n",
      "Train Epoch: 1 [1600/60000 (3%)]\tLoss: -0.648233\n",
      "Train Epoch: 1 [1920/60000 (3%)]\tLoss: -0.603350\n",
      "Train Epoch: 1 [2240/60000 (4%)]\tLoss: -0.602328\n",
      "Train Epoch: 1 [2560/60000 (4%)]\tLoss: -0.624847\n",
      "Train Epoch: 1 [2880/60000 (5%)]\tLoss: -0.579411\n",
      "Train Epoch: 1 [3200/60000 (5%)]\tLoss: -0.569872\n",
      "Train Epoch: 1 [3520/60000 (6%)]\tLoss: -0.665088\n",
      "Train Epoch: 1 [3840/60000 (6%)]\tLoss: -0.600051\n",
      "Train Epoch: 1 [4160/60000 (7%)]\tLoss: -0.609553\n",
      "Train Epoch: 1 [4480/60000 (7%)]\tLoss: -0.591460\n",
      "Train Epoch: 1 [4800/60000 (8%)]\tLoss: -0.604994\n",
      "Train Epoch: 1 [5120/60000 (9%)]\tLoss: -0.597452\n",
      "Train Epoch: 1 [5440/60000 (9%)]\tLoss: -0.613768\n",
      "Train Epoch: 1 [5760/60000 (10%)]\tLoss: -0.613265\n",
      "Train Epoch: 1 [6080/60000 (10%)]\tLoss: -0.673009\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tLoss: -0.603730\n",
      "Train Epoch: 1 [6720/60000 (11%)]\tLoss: -0.599886\n",
      "Train Epoch: 1 [7040/60000 (12%)]\tLoss: -0.613899\n",
      "Train Epoch: 1 [7360/60000 (12%)]\tLoss: -0.584248\n",
      "Train Epoch: 1 [7680/60000 (13%)]\tLoss: -0.609794\n",
      "Train Epoch: 1 [8000/60000 (13%)]\tLoss: -0.584643\n",
      "Train Epoch: 1 [8320/60000 (14%)]\tLoss: -0.653402\n",
      "Train Epoch: 1 [8640/60000 (14%)]\tLoss: -0.618399\n",
      "Train Epoch: 1 [8960/60000 (15%)]\tLoss: -0.613210\n",
      "Train Epoch: 1 [9280/60000 (15%)]\tLoss: -0.593818\n",
      "Train Epoch: 1 [9600/60000 (16%)]\tLoss: -0.620404\n",
      "Train Epoch: 1 [9920/60000 (17%)]\tLoss: -0.640030\n",
      "Train Epoch: 1 [10240/60000 (17%)]\tLoss: -0.603100\n",
      "Train Epoch: 1 [10560/60000 (18%)]\tLoss: -0.612415\n",
      "Train Epoch: 1 [10880/60000 (18%)]\tLoss: -0.608326\n",
      "Train Epoch: 1 [11200/60000 (19%)]\tLoss: -0.642437\n",
      "Train Epoch: 1 [11520/60000 (19%)]\tLoss: -0.623289\n",
      "Train Epoch: 1 [11840/60000 (20%)]\tLoss: -0.643408\n",
      "Train Epoch: 1 [12160/60000 (20%)]\tLoss: -0.627327\n",
      "Train Epoch: 1 [12480/60000 (21%)]\tLoss: -0.625491\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: -0.634488\n",
      "Train Epoch: 1 [13120/60000 (22%)]\tLoss: -0.576506\n",
      "Train Epoch: 1 [13440/60000 (22%)]\tLoss: -0.632391\n",
      "Train Epoch: 1 [13760/60000 (23%)]\tLoss: -0.632288\n",
      "Train Epoch: 1 [14080/60000 (23%)]\tLoss: -0.626690\n",
      "Train Epoch: 1 [14400/60000 (24%)]\tLoss: -0.625963\n",
      "Train Epoch: 1 [14720/60000 (25%)]\tLoss: -0.617509\n",
      "Train Epoch: 1 [15040/60000 (25%)]\tLoss: -0.629243\n",
      "Train Epoch: 1 [15360/60000 (26%)]\tLoss: -0.559844\n",
      "Train Epoch: 1 [15680/60000 (26%)]\tLoss: -0.586447\n",
      "Train Epoch: 1 [16000/60000 (27%)]\tLoss: -0.583837\n",
      "Train Epoch: 1 [16320/60000 (27%)]\tLoss: -0.619522\n",
      "Train Epoch: 1 [16640/60000 (28%)]\tLoss: -0.598037\n",
      "Train Epoch: 1 [16960/60000 (28%)]\tLoss: -0.648591\n",
      "Train Epoch: 1 [17280/60000 (29%)]\tLoss: -0.614331\n",
      "Train Epoch: 1 [17600/60000 (29%)]\tLoss: -0.687739\n",
      "Train Epoch: 1 [17920/60000 (30%)]\tLoss: -0.631900\n",
      "Train Epoch: 1 [18240/60000 (30%)]\tLoss: -0.616753\n",
      "Train Epoch: 1 [18560/60000 (31%)]\tLoss: -0.625363\n",
      "Train Epoch: 1 [18880/60000 (31%)]\tLoss: -0.651668\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tLoss: -0.599474\n",
      "Train Epoch: 1 [19520/60000 (33%)]\tLoss: -0.626223\n",
      "Train Epoch: 1 [19840/60000 (33%)]\tLoss: -0.642546\n",
      "Train Epoch: 1 [20160/60000 (34%)]\tLoss: -0.609432\n",
      "Train Epoch: 1 [20480/60000 (34%)]\tLoss: -0.632828\n",
      "Train Epoch: 1 [20800/60000 (35%)]\tLoss: -0.580182\n",
      "Train Epoch: 1 [21120/60000 (35%)]\tLoss: -0.652703\n",
      "Train Epoch: 1 [21440/60000 (36%)]\tLoss: -0.613714\n",
      "Train Epoch: 1 [21760/60000 (36%)]\tLoss: -0.608907\n",
      "Train Epoch: 1 [22080/60000 (37%)]\tLoss: -0.612515\n",
      "Train Epoch: 1 [22400/60000 (37%)]\tLoss: -0.635109\n",
      "Train Epoch: 1 [22720/60000 (38%)]\tLoss: -0.633547\n",
      "Train Epoch: 1 [23040/60000 (38%)]\tLoss: -0.634792\n",
      "Train Epoch: 1 [23360/60000 (39%)]\tLoss: -0.618439\n",
      "Train Epoch: 1 [23680/60000 (39%)]\tLoss: -0.619535\n",
      "Train Epoch: 1 [24000/60000 (40%)]\tLoss: -0.604408\n",
      "Train Epoch: 1 [24320/60000 (41%)]\tLoss: -0.589582\n",
      "Train Epoch: 1 [24640/60000 (41%)]\tLoss: -0.603924\n",
      "Train Epoch: 1 [24960/60000 (42%)]\tLoss: -0.623405\n",
      "Train Epoch: 1 [25280/60000 (42%)]\tLoss: -0.648729\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: -0.592286\n",
      "Train Epoch: 1 [25920/60000 (43%)]\tLoss: -0.646762\n",
      "Train Epoch: 1 [26240/60000 (44%)]\tLoss: -0.630214\n",
      "Train Epoch: 1 [26560/60000 (44%)]\tLoss: -0.605748\n",
      "Train Epoch: 1 [26880/60000 (45%)]\tLoss: -0.649022\n",
      "Train Epoch: 1 [27200/60000 (45%)]\tLoss: -0.619296\n",
      "Train Epoch: 1 [27520/60000 (46%)]\tLoss: -0.604284\n",
      "Train Epoch: 1 [27840/60000 (46%)]\tLoss: -0.659288\n",
      "Train Epoch: 1 [28160/60000 (47%)]\tLoss: -0.622936\n",
      "Train Epoch: 1 [28480/60000 (47%)]\tLoss: -0.600497\n",
      "Train Epoch: 1 [28800/60000 (48%)]\tLoss: -0.620646\n",
      "Train Epoch: 1 [29120/60000 (49%)]\tLoss: -0.597501\n",
      "Train Epoch: 1 [29440/60000 (49%)]\tLoss: -0.599491\n",
      "Train Epoch: 1 [29760/60000 (50%)]\tLoss: -0.576598\n",
      "Train Epoch: 1 [30080/60000 (50%)]\tLoss: -0.608922\n",
      "Train Epoch: 1 [30400/60000 (51%)]\tLoss: -0.607130\n",
      "Train Epoch: 1 [30720/60000 (51%)]\tLoss: -0.568931\n",
      "Train Epoch: 1 [31040/60000 (52%)]\tLoss: -0.619409\n",
      "Train Epoch: 1 [31360/60000 (52%)]\tLoss: -0.618933\n",
      "Train Epoch: 1 [31680/60000 (53%)]\tLoss: -0.596059\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: -0.666453\n",
      "Train Epoch: 1 [32320/60000 (54%)]\tLoss: -0.612388\n",
      "Train Epoch: 1 [32640/60000 (54%)]\tLoss: -0.646033\n",
      "Train Epoch: 1 [32960/60000 (55%)]\tLoss: -0.567811\n",
      "Train Epoch: 1 [33280/60000 (55%)]\tLoss: -0.623069\n",
      "Train Epoch: 1 [33600/60000 (56%)]\tLoss: -0.615590\n",
      "Train Epoch: 1 [33920/60000 (57%)]\tLoss: -0.645663\n",
      "Train Epoch: 1 [34240/60000 (57%)]\tLoss: -0.631340\n",
      "Train Epoch: 1 [34560/60000 (58%)]\tLoss: -0.622437\n",
      "Train Epoch: 1 [34880/60000 (58%)]\tLoss: -0.607380\n",
      "Train Epoch: 1 [35200/60000 (59%)]\tLoss: -0.630682\n",
      "Train Epoch: 1 [35520/60000 (59%)]\tLoss: -0.642731\n",
      "Train Epoch: 1 [35840/60000 (60%)]\tLoss: -0.628098\n",
      "Train Epoch: 1 [36160/60000 (60%)]\tLoss: -0.588567\n",
      "Train Epoch: 1 [36480/60000 (61%)]\tLoss: -0.629380\n",
      "Train Epoch: 1 [36800/60000 (61%)]\tLoss: -0.570011\n",
      "Train Epoch: 1 [37120/60000 (62%)]\tLoss: -0.604955\n",
      "Train Epoch: 1 [37440/60000 (62%)]\tLoss: -0.608737\n",
      "Train Epoch: 1 [37760/60000 (63%)]\tLoss: -0.641159\n",
      "Train Epoch: 1 [38080/60000 (63%)]\tLoss: -0.637686\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: -0.637451\n",
      "Train Epoch: 1 [38720/60000 (65%)]\tLoss: -0.650599\n",
      "Train Epoch: 1 [39040/60000 (65%)]\tLoss: -0.625717\n",
      "Train Epoch: 1 [39360/60000 (66%)]\tLoss: -0.653219\n",
      "Train Epoch: 1 [39680/60000 (66%)]\tLoss: -0.656284\n",
      "Train Epoch: 1 [40000/60000 (67%)]\tLoss: -0.653389\n",
      "Train Epoch: 1 [40320/60000 (67%)]\tLoss: -0.633587\n",
      "Train Epoch: 1 [40640/60000 (68%)]\tLoss: -0.654359\n",
      "Train Epoch: 1 [40960/60000 (68%)]\tLoss: -0.624273\n",
      "Train Epoch: 1 [41280/60000 (69%)]\tLoss: -0.600671\n",
      "Train Epoch: 1 [41600/60000 (69%)]\tLoss: -0.622884\n",
      "Train Epoch: 1 [41920/60000 (70%)]\tLoss: -0.600670\n",
      "Train Epoch: 1 [42240/60000 (70%)]\tLoss: -0.627441\n",
      "Train Epoch: 1 [42560/60000 (71%)]\tLoss: -0.631499\n",
      "Train Epoch: 1 [42880/60000 (71%)]\tLoss: -0.594237\n",
      "Train Epoch: 1 [43200/60000 (72%)]\tLoss: -0.597340\n",
      "Train Epoch: 1 [43520/60000 (73%)]\tLoss: -0.616454\n",
      "Train Epoch: 1 [43840/60000 (73%)]\tLoss: -0.639905\n",
      "Train Epoch: 1 [44160/60000 (74%)]\tLoss: -0.623356\n",
      "Train Epoch: 1 [44480/60000 (74%)]\tLoss: -0.596409\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tLoss: -0.607713\n",
      "Train Epoch: 1 [45120/60000 (75%)]\tLoss: -0.586125\n",
      "Train Epoch: 1 [45440/60000 (76%)]\tLoss: -0.603805\n",
      "Train Epoch: 1 [45760/60000 (76%)]\tLoss: -0.642440\n",
      "Train Epoch: 1 [46080/60000 (77%)]\tLoss: -0.635622\n",
      "Train Epoch: 1 [46400/60000 (77%)]\tLoss: -0.617844\n",
      "Train Epoch: 1 [46720/60000 (78%)]\tLoss: -0.602670\n",
      "Train Epoch: 1 [47040/60000 (78%)]\tLoss: -0.615638\n",
      "Train Epoch: 1 [47360/60000 (79%)]\tLoss: -0.655057\n",
      "Train Epoch: 1 [47680/60000 (79%)]\tLoss: -0.591797\n",
      "Train Epoch: 1 [48000/60000 (80%)]\tLoss: -0.609839\n",
      "Train Epoch: 1 [48320/60000 (81%)]\tLoss: -0.637095\n",
      "Train Epoch: 1 [48640/60000 (81%)]\tLoss: -0.650761\n",
      "Train Epoch: 1 [48960/60000 (82%)]\tLoss: -0.609262\n",
      "Train Epoch: 1 [49280/60000 (82%)]\tLoss: -0.640476\n",
      "Train Epoch: 1 [49600/60000 (83%)]\tLoss: -0.648550\n",
      "Train Epoch: 1 [49920/60000 (83%)]\tLoss: -0.664491\n",
      "Train Epoch: 1 [50240/60000 (84%)]\tLoss: -0.657181\n",
      "Train Epoch: 1 [50560/60000 (84%)]\tLoss: -0.630243\n",
      "Train Epoch: 1 [50880/60000 (85%)]\tLoss: -0.662174\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: -0.638528\n",
      "Train Epoch: 1 [51520/60000 (86%)]\tLoss: -0.643778\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [51840/60000 (86%)]\tLoss: -0.651197\n",
      "Train Epoch: 1 [52160/60000 (87%)]\tLoss: -0.585497\n",
      "Train Epoch: 1 [52480/60000 (87%)]\tLoss: -0.615759\n",
      "Train Epoch: 1 [52800/60000 (88%)]\tLoss: -0.591854\n",
      "Train Epoch: 1 [53120/60000 (89%)]\tLoss: -0.620696\n",
      "Train Epoch: 1 [53440/60000 (89%)]\tLoss: -0.604362\n",
      "Train Epoch: 1 [53760/60000 (90%)]\tLoss: -0.528934\n",
      "Train Epoch: 1 [54080/60000 (90%)]\tLoss: -0.659793\n",
      "Train Epoch: 1 [54400/60000 (91%)]\tLoss: -0.610089\n",
      "Train Epoch: 1 [54720/60000 (91%)]\tLoss: -0.617942\n",
      "Train Epoch: 1 [55040/60000 (92%)]\tLoss: -0.674422\n",
      "Train Epoch: 1 [55360/60000 (92%)]\tLoss: -0.635013\n",
      "Train Epoch: 1 [55680/60000 (93%)]\tLoss: -0.646340\n",
      "Train Epoch: 1 [56000/60000 (93%)]\tLoss: -0.621533\n",
      "Train Epoch: 1 [56320/60000 (94%)]\tLoss: -0.599598\n",
      "Train Epoch: 1 [56640/60000 (94%)]\tLoss: -0.624114\n",
      "Train Epoch: 1 [56960/60000 (95%)]\tLoss: -0.625199\n",
      "Train Epoch: 1 [57280/60000 (95%)]\tLoss: -0.642815\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tLoss: -0.634345\n",
      "Train Epoch: 1 [57920/60000 (97%)]\tLoss: -0.619605\n",
      "Train Epoch: 1 [58240/60000 (97%)]\tLoss: -0.591487\n",
      "Train Epoch: 1 [58560/60000 (98%)]\tLoss: -0.629899\n",
      "Train Epoch: 1 [58880/60000 (98%)]\tLoss: -0.637101\n",
      "Train Epoch: 1 [59200/60000 (99%)]\tLoss: -0.644076\n",
      "Train Epoch: 1 [59520/60000 (99%)]\tLoss: -0.613413\n",
      "Train Epoch: 1 [59840/60000 (100%)]\tLoss: -0.578129\n",
      "====> Epoch: 1 Average loss: -0.6199\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ren Bettendorf\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\ipykernel_launcher.py:9: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "  if __name__ == '__main__':\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'results/reconstruction_1.png'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-89-9ac89f214348>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mEPOCHS\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mtest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;31m# 64 sets of random ZDIMS-float vectors, i.e. 64 locations / MNIST\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-88-650552ccf721>\u001b[0m in \u001b[0;36mtest\u001b[1;34m(epoch)\u001b[0m\n\u001b[0;32m     17\u001b[0m                                   recon_batch.view(BATCH_SIZE, 1, 28, 28)[:n]])\n\u001b[0;32m     18\u001b[0m           save_image(comparison.data.cpu(),\n\u001b[1;32m---> 19\u001b[1;33m                      'results/reconstruction_' + str(epoch) + '.png', nrow=n)\n\u001b[0m\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m     \u001b[0mtest_loss\u001b[0m \u001b[1;33m/=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_loader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\torchvision\\utils.py\u001b[0m in \u001b[0;36msave_image\u001b[1;34m(tensor, filename, nrow, padding, normalize, range, scale_each, pad_value)\u001b[0m\n\u001b[0;32m    103\u001b[0m     \u001b[0mndarr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgrid\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m255\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0.5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclamp_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m255\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'cpu'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muint8\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    104\u001b[0m     \u001b[0mim\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfromarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mndarr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 105\u001b[1;33m     \u001b[0mim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\PIL\\Image.py\u001b[0m in \u001b[0;36msave\u001b[1;34m(self, fp, format, **params)\u001b[0m\n\u001b[0;32m   2002\u001b[0m                 \u001b[1;31m# Open also for reading (\"+\"), because TIFF save_all\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2003\u001b[0m                 \u001b[1;31m# writer needs to go back and edit the written data.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2004\u001b[1;33m                 \u001b[0mfp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbuiltins\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"w+b\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2005\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2006\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'results/reconstruction_1.png'"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, EPOCHS + 1):\n",
    "    train(epoch)\n",
    "    test(epoch)\n",
    "\n",
    "    # 64 sets of random ZDIMS-float vectors, i.e. 64 locations / MNIST\n",
    "    # digits in latent space\n",
    "    sample = Variable(torch.randn(64, 20))\n",
    "    sample = model.decode(sample).cpu()\n",
    "\n",
    "    # save out as an 8x8 matrix of MNIST digits\n",
    "    # this will give you a visual idea of how well latent space can generate things\n",
    "    # that look like digits\n",
    "    save_image(sample.data.view(64, 1, 28, 28),\n",
    "               'results/sample_' + str(epoch) + '.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-ml3F2uSh7-p"
   },
   "source": [
    "## **Part 4: Visualizing the VAE output** (90 points)\n",
    "\n",
    "We will look at how well the codes produced by the VAE can be interpolated. **For this section we will only use the MNIST test set. **\n",
    "\n",
    "To create an interpolation between two images A and B, we encode both these images and generate a sample code for each of them. We now consider 7 equally spaced points in between these two sample codes giving us a total of 9 points including the samples. We then decode these images to get interpolated images in between A and B.\n",
    "\n",
    "Complete the interpolation function below that takes a pair of images A and B and returns 9 images. (You are free to use any data structure you want to return these images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JRE_LDjNjnAX"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from torchvision import utils\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "\n",
    "def create_interpolates(A, B, encoder, decoder):\n",
    "  ## YOUR CODE HERE ##\n",
    "  return "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uCBT8zvEk_zn"
   },
   "source": [
    "**For 10 pairs of MNIST test images of the same digit (1 pair for \"0\", 1 pair for \"1\", etc.), selected at random, compute the code for each image of the pair. Now compute 7 evenly spaced linear interpolates between these codes, and decode the result into images. Prepare a figure showing this interpolate. Lay out the figure so each interpolate is a row. On the left of the row is the first test image; then the interpolate closest to it; etc; to the last test image. You should have a 10 rows (1 row per digit) and 9 columns (7 interpolates + 2 selected test images) of images. (45 points)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zoaMBIu0jABd"
   },
   "outputs": [],
   "source": [
    "similar_pairs = {}\n",
    "for _, (x, y) in enumerate(test_loader):\n",
    "  for i in range(len(y)):\n",
    "    if y[i].item() not in similar_pairs:\n",
    "      similar_pairs[y[i].item()] = []\n",
    "    if len(similar_pairs[y[i].item()])<2:\n",
    "      similar_pairs[y[i].item()].append(x[i])\n",
    "  \n",
    "  done = True\n",
    "  for i in range(10):\n",
    "    if i not in similar_pairs or len(similar_pairs[i])<2:\n",
    "      done = False\n",
    "  \n",
    "  if done:\n",
    "    break\n",
    "\n",
    "# similar_pairs[i] contains two images indexed at 0 and 1 that have images of the digit i\n",
    "\n",
    "## YOUR CODE HERE ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "k4q7-6jTlQj9"
   },
   "source": [
    "**For 10 pairs of MNIST test images, selected at random, compute the code for each image of the pair. Now compute 7 evenly spaced linear interpolates between these codes, and decode the result into images. Prepare a figure showing this interpolate. Lay out the figure so each interpolate is a row. On the left of the row is the first test image; then the interpolate closest to it; etc; to the last test image. You should have a 10 rows and 9 columns of images. (45 points)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iKgDR1L-lU11"
   },
   "outputs": [],
   "source": [
    "random_pairs = {}\n",
    "for _, (x, y) in enumerate(test_loader):\n",
    "  # Make sure the batch size is greater than 20\n",
    "  for i in range(10):\n",
    "    random_pairs[i] = []\n",
    "    random_pairs[i].append(x[2*i])\n",
    "    random_pairs[i].append(x[2*i+1])\n",
    "  break\n",
    "\n",
    "# random_pairs[i] contains two images indexed at 0 and 1 that are chosen at random\n",
    "\n",
    "## YOUR CODE HERE ##"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "HW9.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
